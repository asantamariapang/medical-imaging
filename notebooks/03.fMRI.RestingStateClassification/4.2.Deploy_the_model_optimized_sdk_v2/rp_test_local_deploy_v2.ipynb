{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install azure-ai-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    OnlineRequestSettings\n",
        ")\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# enter details of your AML workspace\n",
        "subscription_id = \"1e9b4bc4-253a-40c3-8771-998507855894\"\n",
        "resource_group = \"user-manageg-key-test\"\n",
        "workspace = \"nuance_benchmark\"\n",
        "\n",
        "# get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configure an environment\n",
        "env = Environment(\n",
        "    conda_file=\"conda_dep_opti.yml\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    )\n",
        "\n",
        "# configure an inference configuration with a scoring script\n",
        "code_config = CodeConfiguration(\n",
        "        code=\"fmri_score_code\",\n",
        "        scoring_script=\"score_opti.py\"\n",
        "    )   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# folder_data_model_path=\"../fmri-data-pt-onnx-ov-models\"\n",
        "\n",
        "# file_model = Model(\n",
        "#     path=folder_data_model_path,\n",
        "#     type=AssetTypes.CUSTOM_MODEL,\n",
        "#     name=\"fmri-data-pt-onnx-ov-v2sdk\",\n",
        "#     version=\"1\",\n",
        "#     description=\"SDKv2-fmri-data-pt-onnx-ov-models with PT, ONNX and OV models of fMRI - final25D model. Also includes 100 IC_niftis test volumes (*.nii.gz)\"\n",
        "# )\n",
        "# ml_client.models.create_or_update(file_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_data_model_path=\"../fmri-data-pt-onnx-ov-models\"\n",
        "\n",
        "local_model = Model(\n",
        "    path=folder_data_model_path,\n",
        "    type=AssetTypes.CUSTOM_MODEL,\n",
        "    name=\"fmri-data-pt-onnx-ov-v2sdk\",\n",
        "    version=\"1\",\n",
        "    description=\"SDKv2-fmri-data-pt-onnx-ov-models with PT, ONNX and OV models of fMRI - final25D model. Also includes 100 IC_niftis test volumes (*.nii.gz)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Updating local endpoint (fmri-pt-ipex-ov-local-sdk-v2) .Done (0m 5s)\n"
          ]
        }
      ],
      "source": [
        "online_endpoint_name = \"fmri-pt-ipex-ov-local-sdk-v2\"\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name = online_endpoint_name, \n",
        "    description=\"this is local: fmri-pt-ipex-ov-local-sdk-v2\",\n",
        "    auth_mode=\"key\"\n",
        ")\n",
        "\n",
        "poller = ml_client.online_endpoints.begin_create_or_update(endpoint, local=True)\n",
        "#poller.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Updating local deployment (fmri-pt-ipex-ov-local-sdk-v2 / blue) .\n",
            "Building Docker image from Dockerfile\n",
            "Step 1/6 : FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\n",
            " ---> 2a70613828e9\n",
            "Step 2/6 : RUN mkdir -p /var/azureml-app/\n",
            " ---> Using cache\n",
            " ---> b9e650e0cd57\n",
            "Step 3/6 : WORKDIR /var/azureml-app/\n",
            " ---> Using cache\n",
            " ---> dd0dc0799daf\n",
            "Step 4/6 : COPY conda.yml /var/azureml-app/\n",
            " ---> Using cache\n",
            " ---> 53af016f6d16\n",
            "Step 5/6 : RUN conda env create -n inf-conda-env --file conda.yml\n",
            " ---> Using cache\n",
            " ---> 1768010f34bd\n",
            "Step 6/6 : CMD [\"conda\", \"run\", \"--no-capture-output\", \"-n\", \"inf-conda-env\", \"runsvdir\", \"/var/runit\"]\n",
            " ---> Using cache\n",
            " ---> 1c963946291f\n",
            "Successfully built 1c963946291f\n",
            "Successfully tagged fmri-pt-ipex-ov-local-sdk-v2:blue\n",
            "\n",
            "Starting up endpoint...Done (0m 20s)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ManagedOnlineDeployment({'private_network_connection': None, 'provisioning_state': 'Succeeded', 'endpoint_name': 'fmri-pt-ipex-ov-local-sdk-v2', 'type': 'Managed', 'name': 'blue', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/ravi-nuance/code/Users/ravi.panchumarthy/03.fMRI.RestingStateClassification/4.2.Deploy_the_model(optimized)_sdk_v2'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fb025051460>, 'model': Model({'job_name': None, 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'fmri-data-pt-onnx-ov-v2sdk', 'description': 'SDKv2-fmri-data-pt-onnx-ov-models with PT, ONNX and OV models of fMRI - final25D model. Also includes 100 IC_niftis test volumes (*.nii.gz)', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/ravi-nuance/code/Users/ravi.panchumarthy/03.fMRI.RestingStateClassification/4.2.Deploy_the_model(optimized)_sdk_v2'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fb025054c10>, 'version': '1', 'latest_version': None, 'path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ravi-nuance/code/Users/ravi.panchumarthy/03.fMRI.RestingStateClassification/fmri-data-pt-onnx-ov-models', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model'}), 'code_configuration': {'code': 'fmri_score_code'}, 'environment': Environment({'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'CliV2AnonymousEnvironment', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/ravi-nuance/code/Users/ravi.panchumarthy/03.fMRI.RestingStateClassification/4.2.Deploy_the_model(optimized)_sdk_v2'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fb025047c10>, 'version': 'b3364ebb2398cbd6ce2b7bfac95c5c78', 'latest_version': None, 'conda_file': {'channels': ['anaconda', 'defaults'], 'dependencies': [{'pip': ['azureml-defaults', 'azure-ml-api-sdk', 'nibabel', 'scikit-learn', 'scipy', 'pandas', 'openvino-dev', 'torch==1.13.1+cpu', 'torchvision==0.14.1+cpu', 'intel_extension_for_pytorch==1.13.100', 'torchio', '--index-url https://pypi.org/simple/', '--extra-index-url https://download.pytorch.org/whl/cpu']}]}, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest', 'build': None, 'inference_config': None, 'os_type': None, 'arm_type': 'environment_version', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': 'channels:\\n- anaconda\\n- defaults\\ndependencies:\\n- pip:\\n  - azureml-defaults\\n  - azure-ml-api-sdk\\n  - nibabel\\n  - scikit-learn\\n  - scipy\\n  - pandas\\n  - openvino-dev\\n  - torch==1.13.1+cpu\\n  - torchvision==0.14.1+cpu\\n  - intel_extension_for_pytorch==1.13.100\\n  - torchio\\n  - --index-url https://pypi.org/simple/\\n  - --extra-index-url https://download.pytorch.org/whl/cpu\\n'}), 'environment_variables': {}, 'app_insights_enabled': False, 'scale_settings': None, 'request_settings': <azure.ai.ml.entities._deployment.deployment_settings.OnlineRequestSettings object at 0x7fb025047ee0>, 'liveness_probe': None, 'readiness_probe': None, 'instance_count': 1, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'local', 'data_collector': None, 'egress_public_network_access': None})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "req_settings = OnlineRequestSettings(request_timeout_ms=36000)\n",
        "\n",
        "# Define a deployment\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=local_model,\n",
        "    environment=env,\n",
        "    code_configuration=code_config,\n",
        "    instance_type=\"Standard_FX4mds\", #Standard_FX4mds, Standard_F2s_v2\n",
        "    instance_count=1,\n",
        "    request_settings=req_settings\n",
        ")\n",
        "\n",
        "\n",
        "# create the deployment:\n",
        "ml_client.begin_create_or_update(blue_deployment, local=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Updating local endpoint (fmri-pt-ipex-ov-local-sdk-v2) .Done (0m 5s)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ManagedOnlineEndpoint({'public_network_access': None, 'provisioning_state': 'Succeeded', 'scoring_uri': 'http://localhost:32782/score', 'openapi_uri': None, 'name': 'fmri-pt-ipex-ov-local-sdk-v2', 'description': 'this is local: fmri-pt-ipex-ov-local-sdk-v2', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/ravi-nuance/code/Users/ravi.panchumarthy/03.fMRI.RestingStateClassification/4.2.Deploy_the_model(optimized)_sdk_v2'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fb024ff1940>, 'auth_mode': 'key', 'location': 'local', 'identity': None, 'traffic': {'blue': 100}, 'mirror_traffic': {}, 'kind': None})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# blue deployment takes 100% traffic\n",
        "endpoint.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(endpoint, local=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ManagedOnlineEndpoint({'public_network_access': None, 'provisioning_state': 'Succeeded', 'scoring_uri': 'http://localhost:32782/score', 'openapi_uri': None, 'name': 'fmri-pt-ipex-ov-local-sdk-v2', 'description': 'this is local: fmri-pt-ipex-ov-local-sdk-v2', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/ravi-nuance/code/Users/ravi.panchumarthy/03.fMRI.RestingStateClassification/4.2.Deploy_the_model(optimized)_sdk_v2'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fb02c09b9a0>, 'auth_mode': 'key', 'location': 'local', 'identity': None, 'traffic': {'blue': 100}, 'mirror_traffic': {}, 'kind': None})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ml_client.online_endpoints.get(name=online_endpoint_name, local=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2023-06-01T23:23:37,184473954+00:00 - rsyslog/run \\r\\n2023-06-01T23:23:37,186520478+00:00 - nginx/run \\r\\n2023-06-01T23:23:37,187539790+00:00 - gunicorn/run \\r\\n2023-06-01T23:23:37,188979107+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:37,190280022+00:00 | gunicorn/run | ###############################################\\r\\n2023-06-01T23:23:37,191584637+00:00 | gunicorn/run | AzureML Container Runtime Information\\r\\n2023-06-01T23:23:37,193198556+00:00 | gunicorn/run | ###############################################\\r\\n2023-06-01T23:23:37,194501972+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:37,476509093+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:37,479162725+00:00 | gunicorn/run | AzureML image information: openmpi4.1.0-ubuntu20.04, Materializaton Build:20230530.v2\\r\\n2023-06-01T23:23:37,480442240+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:37,481775655+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:37,483146671+00:00 | gunicorn/run | PATH environment variable: /opt/miniconda/envs/inf-conda-env/bin:/opt/miniconda/condabin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\r\\n2023-06-01T23:23:37,484471787+00:00 | gunicorn/run | PYTHONPATH environment variable: \\r\\n2023-06-01T23:23:37,485757202+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:37,764104881+00:00 | gunicorn/run | CONDAPATH environment variable: /opt/miniconda\\r\\n\\r\\n# conda environments:\\r\\n#\\r\\nbase                     /opt/miniconda\\r\\ninf-conda-env         *  /opt/miniconda/envs/inf-conda-env\\r\\n\\r\\n2023-06-01T23:23:38,325092588+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:38,326460704+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)\\r\\n\\r\\nadal==1.2.7\\r\\naddict==2.4.0\\r\\nargcomplete==2.1.2\\r\\nattrs==23.1.0\\r\\nazure-common==1.1.28\\r\\nazure-core==1.27.0\\r\\nazure-graphrbac==0.61.1\\r\\nazure-identity==1.13.0\\r\\nazure-mgmt-authorization==3.0.0\\r\\nazure-mgmt-containerregistry==10.1.0\\r\\nazure-mgmt-core==1.4.0\\r\\nazure-mgmt-keyvault==10.2.2\\r\\nazure-mgmt-resource==22.0.0\\r\\nazure-mgmt-storage==21.0.0\\r\\nazure-ml-api-sdk==0.1.0a11\\r\\nazureml-core==1.51.0\\r\\nazureml-dataprep==4.10.8\\r\\nazureml-dataprep-native==38.0.0\\r\\nazureml-dataprep-rslex==2.17.12\\r\\nazureml-dataset-runtime==1.51.0\\r\\nazureml-defaults==1.51.0\\r\\nazureml-inference-server-http==0.8.4\\r\\nbackports.tempfile==1.0\\r\\nbackports.weakref==1.0.post1\\r\\nbcrypt==4.0.1\\r\\ncachetools==5.3.1\\r\\ncertifi @ file:///croot/certifi_1671487769961/work/certifi\\r\\ncffi==1.15.1\\r\\ncharset-normalizer==3.1.0\\r\\nclick==8.1.3\\r\\ncloudpickle==2.2.1\\r\\ncolorama==0.4.6\\r\\ncontextlib2==21.6.0\\r\\ncryptography==40.0.2\\r\\ndefusedxml==0.7.1\\r\\nDeprecated==1.2.14\\r\\ndistro==1.8.0\\r\\ndocker==6.1.3\\r\\ndotnetcore2==3.1.23\\r\\nFlask==2.2.5\\r\\nFlask-Cors==3.0.10\\r\\nfusepy==3.0.1\\r\\ngoogle-api-core==2.11.0\\r\\ngoogle-auth==2.19.1\\r\\ngoogleapis-common-protos==1.59.0\\r\\ngunicorn==20.1.0\\r\\nhumanfriendly==10.0\\r\\nhumanize==4.6.0\\r\\nidna==3.4\\r\\ninference-schema==1.5.1\\r\\nintel-extension-for-pytorch==1.13.100\\r\\nisodate==0.6.1\\r\\nitsdangerous==2.1.2\\r\\njeepney==0.8.0\\r\\nJinja2==3.1.2\\r\\njmespath==1.0.1\\r\\njoblib==1.2.0\\r\\njsonpickle==3.0.1\\r\\njsonschema==4.17.3\\r\\njstyleson==0.0.2\\r\\nknack==0.10.1\\r\\nliac-arff==2.5.0\\r\\nmarkdown-it-py==2.2.0\\r\\nMarkupSafe==2.1.2\\r\\nmdurl==0.1.2\\r\\nmsal==1.22.0\\r\\nmsal-extensions==1.0.0\\r\\nmsrest==0.7.1\\r\\nmsrestazure==0.6.4\\r\\nndg-httpsclient==0.5.1\\r\\nnetworkx==2.8.8\\r\\nnibabel==5.1.0\\r\\nnumpy==1.23.5\\r\\noauthlib==3.2.2\\r\\nopencensus==0.11.2\\r\\nopencensus-context==0.1.3\\r\\nopencensus-ext-azure==1.1.9\\r\\nopencv-python==4.7.0.72\\r\\nopenvino==2023.0.0\\r\\nopenvino-dev==2023.0.0\\r\\nopenvino-telemetry==2022.3.0\\r\\npackaging==23.0\\r\\npandas==2.0.2\\r\\nparamiko==3.2.0\\r\\npathspec==0.11.1\\r\\nPillow==9.5.0\\r\\npkginfo==1.9.6\\r\\nportalocker==2.7.0\\r\\nprotobuf==4.23.2\\r\\npsutil==5.9.5\\r\\npyarrow==9.0.0\\r\\npyasn1==0.5.0\\r\\npyasn1-modules==0.3.0\\r\\npycparser==2.21\\r\\npydantic==1.10.8\\r\\nPygments==2.15.1\\r\\nPyJWT==2.7.0\\r\\nPyNaCl==1.5.0\\r\\npyOpenSSL==23.2.0\\r\\npyrsistent==0.19.3\\r\\nPySocks==1.7.1\\r\\npython-dateutil==2.8.2\\r\\npytz==2023.3\\r\\nPyYAML==6.0\\r\\nrequests==2.31.0\\r\\nrequests-oauthlib==1.3.1\\r\\nrich==13.4.1\\r\\nrsa==4.9\\r\\nscikit-learn==1.2.2\\r\\nscipy==1.10.1\\r\\nSecretStorage==3.3.3\\r\\nshellingham==1.5.0.post1\\r\\nSimpleITK==2.2.1\\r\\nsix==1.16.0\\r\\ntabulate==0.9.0\\r\\ntexttable==1.6.7\\r\\nthreadpoolctl==3.1.0\\r\\ntorch==1.13.1+cpu\\r\\ntorchio==0.18.91\\r\\ntorchvision==0.14.1+cpu\\r\\ntqdm==4.65.0\\r\\ntyper==0.9.0\\r\\ntyping_extensions==4.6.2\\r\\ntzdata==2023.3\\r\\nurllib3==1.26.16\\r\\nwebsocket-client==1.5.2\\r\\nWerkzeug==2.3.4\\r\\nwrapt==1.12.1\\r\\n\\r\\n2023-06-01T23:23:38,757715984+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:38,759224402+00:00 | gunicorn/run | ###############################################\\r\\n2023-06-01T23:23:38,760624518+00:00 | gunicorn/run | Checking if the Python package azureml-inference-server-http is installed\\r\\n2023-06-01T23:23:38,761893833+00:00 | gunicorn/run | ###############################################\\r\\n2023-06-01T23:23:38,763271049+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:39,666618389+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:39,668368010+00:00 | gunicorn/run | ###############################################\\r\\n2023-06-01T23:23:39,670027329+00:00 | gunicorn/run | AzureML Inference Server\\r\\n2023-06-01T23:23:39,671648549+00:00 | gunicorn/run | ###############################################\\r\\n2023-06-01T23:23:39,673271068+00:00 | gunicorn/run | \\r\\n2023-06-01T23:23:40,641950877+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\\r\\n2023-06-01 23:23:40,780 I [22] azmlinfsrv - Loaded logging config from /opt/miniconda/envs/inf-conda-env/lib/python3.10/site-packages/azureml_inference_server_http/logging.json\\r\\n\\r\\nAzure ML Inferencing HTTP server v0.8.4\\r\\n\\r\\n\\r\\nServer Settings\\r\\n---------------\\r\\nEntry Script Name: /var/azureml-app/fmri_score_code/score_opti.py\\r\\nModel Directory: /var/azureml-app/azureml-models//fmri-data-pt-onnx-ov-v2sdk/1\\r\\nConfig File: None\\r\\nWorker Count: 1\\r\\nWorker Timeout (seconds): 300\\r\\nServer Port: 31311\\r\\nHealth Port: 31311\\r\\nApplication Insights Enabled: false\\r\\nApplication Insights Key: None\\r\\nInferencing HTTP server version: azmlinfsrv/0.8.4\\r\\nCORS for the specified origins: None\\r\\nCreate dedicated endpoint for health: None\\r\\n\\r\\n\\r\\nServer Routes\\r\\n---------------\\r\\nLiveness Probe: GET   127.0.0.1:31311/\\r\\nScore:          POST  127.0.0.1:31311/score\\r\\n\\r\\n2023-06-01 23:23:40,926 I [22] gunicorn.error - Starting gunicorn 20.1.0\\r\\n2023-06-01 23:23:40,927 I [22] gunicorn.error - Listening at: http://0.0.0.0:31311 (22)\\r\\n2023-06-01 23:23:40,927 I [22] gunicorn.error - Using worker: sync\\r\\n2023-06-01 23:23:40,929 I [84] gunicorn.error - Booting worker with pid: 84\\r\\n/opt/miniconda/envs/inf-conda-env/lib/python3.10/site-packages/azureml_inference_server_http/server/config.py:51: FutureWarning: aliases are no longer used by BaseSettings to define which environment variables to read. Instead use the \"env\" field setting. See https://pydantic-docs.helpmanual.io/usage/settings/#environment-variable-names\\r\\n  class AMLInferenceServerConfig(pydantic.BaseSettings):\\r\\n2023-06-01 23:23:41,500 I [84] azmlinfsrv - AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\\r\\nInitializing logger\\r\\n2023-06-01 23:23:41,502 I [84] azmlinfsrv - Starting up app insights client\\r\\n2023-06-01 23:23:43,796 I [84] azmlinfsrv.user_script - Found user script at /var/azureml-app/fmri_score_code/score_opti.py\\r\\n2023-06-01 23:23:43,796 I [84] azmlinfsrv.user_script - run() is decorated with @rawhttp. Server will invoke it with the flask request object.\\r\\n2023-06-01 23:23:43,796 I [84] azmlinfsrv.user_script - Invoking user\\'s init function\\r\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ml_client.online_deployments.get_logs(\n",
        "    name=\"blue\", endpoint_name=online_endpoint_name, local=True, lines=500\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'blue': 100}\n",
            "http://localhost:32782/score\n"
          ]
        }
      ],
      "source": [
        "# Get the details for online endpoint\n",
        "endpoint_deployed = ml_client.online_endpoints.get(name=online_endpoint_name, local=True)\n",
        "\n",
        "# existing traffic details\n",
        "print(endpoint_deployed.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(endpoint_deployed.scoring_uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "ConnectionError",
          "evalue": "HTTPConnectionPool(host='localhost', port=32782): Max retries exceeded with url: /score (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb00fcf4610>: Failed to establish a new connection: [Errno 111] Connection refused'))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[39mraise\u001b[39;00m socket\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mgetaddrinfo returns an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    400\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/http/client.py:1255\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1255\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/http/client.py:1301\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1300\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1301\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/http/client.py:1250\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1250\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/http/client.py:1010\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1010\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1012\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m \n\u001b[1;32m   1014\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/http/client.py:950\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 950\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    951\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_conn(conn)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fb00fcf4610>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=32782): Max retries exceeded with url: /score (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb00fcf4610>: Failed to establish a new connection: [Errno 111] Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m scoring_uri \u001b[39m=\u001b[39m endpoint_deployed\u001b[39m.\u001b[39mscoring_uri\n\u001b[1;32m      9\u001b[0m \u001b[39m# Send the DICOM as a raw HTTP request and obtain results from endpoint.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(scoring_uri, files\u001b[39m=\u001b[39;49mfiles)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput:\u001b[39m\u001b[39m\"\u001b[39m, response\u001b[39m.\u001b[39mcontent)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/requests/adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=32782): Max retries exceeded with url: /score (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb00fcf4610>: Failed to establish a new connection: [Errno 111] Connection refused'))"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "test_file = \"./sample_dicom.dcm\"\n",
        "files = {'image': open(test_file, 'rb').read()}\n",
        "\n",
        "# resp = requests.post(scoring_uri, input_data, headers=headers)\n",
        "scoring_uri = endpoint_deployed.scoring_uri\n",
        "\n",
        "# Send the DICOM as a raw HTTP request and obtain results from endpoint.\n",
        "response = requests.post(scoring_uri, files=files)\n",
        "print(\"output:\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "output_dict = json.loads(response.content)\n",
        "\n",
        "pt_metrics = output_dict['pt_summary']\n",
        "ipex_metrics = output_dict['ipex_summary']\n",
        "ov_metrics = output_dict['ov_summary']\n",
        "\n",
        "print(f\"PyTorch Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{output_dict['system_info']['fwk_versions']['PyTorch']}\")\n",
        "print(f\"\\tTop Labels:\\t{pt_metrics['pt_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{pt_metrics['pt_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{pt_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{pt_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"\\nIPEX Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{output_dict['system_info']['fwk_versions']['IPEX']}\")\n",
        "print(f\"\\tTop Labels:\\t{ipex_metrics['ipex_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{ipex_metrics['ipex_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{ipex_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{ipex_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"\\nOpenVINO Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{output_dict['system_info']['fwk_versions']['OpenVINO']}\")\n",
        "print(f\"\\tTop Labels:\\t{ov_metrics['ov_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{ov_metrics['ov_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{ov_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{ov_metrics['fps']:.2f}\")\n",
        "\n",
        "# Calculate the FPS speedup with IPEX compared to PyTorch\n",
        "ipex_fps_speedup = ipex_metrics['fps'] / pt_metrics['fps']\n",
        "print(f\"\\nSpeedup with IPEX: {ipex_fps_speedup:.2f}x\")\n",
        "\n",
        "# Calculate the FPS speedup with OpenVINO compared to PyTorch\n",
        "ov_fps_speedup = ov_metrics['fps'] / pt_metrics['fps']\n",
        "print(f\"\\nSpeedup with OpenVINO: {ov_fps_speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Print System info\n",
        "lscpu_out=output_dict['system_info']['lscpu_out'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Info:\\n{lscpu_out}\")\n",
        "\n",
        "mem_out_gb=output_dict['system_info']['mem_out_gb'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Memory Info (GB):\\n{mem_out_gb}\")\n",
        "\n",
        "os_out=output_dict['system_info']['os'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem OS:\\n{os_out}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
