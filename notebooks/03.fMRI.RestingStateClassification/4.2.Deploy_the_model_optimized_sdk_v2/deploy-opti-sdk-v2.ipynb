{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy a model to an online endpoint, using Azure Machine Learning Python SDK v2.\n",
        "### Example with fMRI use case\n",
        "For reference, [click here](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-deploy-model?view=azureml-api-2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install azure-ai-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    OnlineRequestSettings\n",
        ")\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# enter details of your AML workspace\n",
        "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
        "resource_group = \"<RESOURCE_GROUP>\"\n",
        "workspace_name = \"<AML_WORKSPACE_NAME>\"\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=subscription_id,\n",
        "    resource_group_name=resource_group,\n",
        "    workspace_name=workspace_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "endpoint_name = \"fmri-pt-ipex-ov-sdk-v2-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name = endpoint_name, \n",
        "    description=\"this is online endpoint: fmri-pt-ipex-ov-sdk-v2\",\n",
        "    auth_mode=\"key\"\n",
        ")\n",
        "\n",
        "poller = ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
        "poller.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure a model\n",
        "\n",
        "folder_data_model_path=\"../fmri-data-pt-onnx-ov-models\"\n",
        "\n",
        "model = Model(\n",
        "    path=folder_data_model_path,\n",
        "    type=AssetTypes.CUSTOM_MODEL,\n",
        "    name=\"fmri-data-pt-onnx-ov-v2sdk\",\n",
        "    version=\"1\",\n",
        "    description=\"SDKv2-fmri-data-pt-onnx-ov-models with PT, ONNX and OV models of fMRI - final25D model. Also includes 100 IC_niftis test volumes (*.nii.gz)\"\n",
        ")\n",
        "ml_client.models.create_or_update(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure an environment\n",
        "env = Environment(\n",
        "    conda_file=\"conda_dep_opti.yml\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    )\n",
        "\n",
        "# configure an inference configuration with a scoring script\n",
        "code_config = CodeConfiguration(\n",
        "        code=\"fmri_score_code\",\n",
        "        scoring_script=\"score_opti-bench.py\"\n",
        "    )   "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Deployment\n",
        "See VM SKUs that are supported for Azure Machine Learning managed online endpoints [here](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list?view=azureml-api-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint fmri-pt-ipex-ov-sdk-v2-1 exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".........................................................................................................."
          ]
        }
      ],
      "source": [
        "\n",
        "req_settings = OnlineRequestSettings(request_timeout_ms=90000)  # 90000ms = 1.5min\n",
        "\n",
        "# Define a deployment\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=model,\n",
        "    environment=env,\n",
        "    code_configuration=code_config,\n",
        "    instance_type=\"Standard_FX4mds\", #Standard_FX4mds, Standard_FX12mds, Standard_F2s_v2\n",
        "    instance_count=1,\n",
        "    request_settings=req_settings\n",
        ")\n",
        "\n",
        "# create the deployment:\n",
        "poller = ml_client.begin_create_or_update(blue_deployment)\n",
        "poller.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<azure.core.polling._poller.LROPoller at 0x7f296d97b550>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set blue deployment to take 100% traffic\n",
        "endpoint.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Deployment logs if needed.\n",
        "deployment_logs = ml_client.online_deployments.get_logs(\n",
        "    name=\"blue\", endpoint_name=endpoint_name, lines=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'blue': 100}\n",
            "https://fmri-pt-ipex-ov-sdk-v2-1.eastus.inference.ml.azure.com/score\n",
            "Authkey:XpnFG6LGTY...\n"
          ]
        }
      ],
      "source": [
        "# Get the details for online endpoint\n",
        "deployed_endpoint = ml_client.online_endpoints.get(name=endpoint_name)\n",
        "\n",
        "# existing traffic details\n",
        "print(deployed_endpoint.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(deployed_endpoint.scoring_uri)\n",
        "\n",
        "auth_key = ml_client.online_endpoints.get_keys(endpoint_name).primary_key\n",
        "print(f\"Authkey:{auth_key[:10]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# resp = requests.post(scoring_uri, input_data, headers=headers)\n",
        "scoring_uri = deployed_endpoint.scoring_uri\n",
        "\n",
        "# Send HTTP request and obtain results from endpoint. \n",
        "# Note: in this example, the input data is already in the container uploaded along with the models during model registration.\n",
        "response = requests.post(scoring_uri, headers={\"Authorization\": f\"Bearer {auth_key}\"}, timeout=600)\n",
        "output_dict = json.loads(response.content)\n",
        "print(json.dumps(output_dict, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Metrics:\n",
            "\tFramework Version:\t1.13.1+cpu\n",
            "\tNum Subjects:\t3600\n",
            "\tTest Accuracy:\t99.47222222222223\n",
            "\tTime Taken:\t21.9458 sec\n",
            "\n",
            "IPEX Metrics:\n",
            "\tFramework Version:\t1.13.100\n",
            "\tNum Subjects:\t3600\n",
            "\tTest Accuracy:\t99.47222222222223\n",
            "\tTime Taken:\t13.6719 sec\n",
            "\n",
            "OpenVINO Metrics:\n",
            "\tFramework Version:\t2023.0.0-10926-b4452d56304-releases/2023/0\n",
            "\tNum Subjects:\t3600\n",
            "\tTest Accuracy:\t99.47222222222223\n",
            "\tTime Taken:\t13.1644 sec\n",
            "\n",
            "Speedup with IPEX: 1.61x\n",
            "\n",
            "Speedup with OpenVINO: 1.67x\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "output_dict = json.loads(response.content)\n",
        "\n",
        "pt_metrics = output_dict['pt_summary']\n",
        "ipex_metrics = output_dict['ipex_summary']\n",
        "ov_metrics = output_dict['ov_summary']\n",
        "\n",
        "print(f\"PyTorch Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{output_dict['system_info']['fwk_versions']['PyTorch']}\")\n",
        "print(f\"\\tNum Subjects:\\t{pt_metrics['num_subjects']}\")\n",
        "print(f\"\\tTest Accuracy:\\t{pt_metrics['test_accuracy']}\")\n",
        "print(f\"\\tTime Taken:\\t{pt_metrics['time_sec']:.4f} sec\")\n",
        "\n",
        "\n",
        "print(f\"\\nIPEX Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{output_dict['system_info']['fwk_versions']['IPEX']}\")\n",
        "print(f\"\\tNum Subjects:\\t{ipex_metrics['num_subjects']}\")\n",
        "print(f\"\\tTest Accuracy:\\t{ipex_metrics['test_accuracy']}\")\n",
        "print(f\"\\tTime Taken:\\t{ipex_metrics['time_sec']:.4f} sec\")\n",
        "\n",
        "print(f\"\\nOpenVINO Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{output_dict['system_info']['fwk_versions']['OpenVINO']}\")\n",
        "print(f\"\\tNum Subjects:\\t{ov_metrics['num_subjects']}\")\n",
        "print(f\"\\tTest Accuracy:\\t{ov_metrics['test_accuracy']}\")\n",
        "print(f\"\\tTime Taken:\\t{ov_metrics['time_sec']:.4f} sec\")\n",
        "\n",
        "# Calculate the speedup with IPEX compared to PyTorch\n",
        "ipex_fps_speedup = pt_metrics['time_sec'] / ipex_metrics['time_sec']\n",
        "print(f\"\\nSpeedup with IPEX: {ipex_fps_speedup:.2f}x\")\n",
        "\n",
        "# Calculate the speedup with OpenVINO compared to PyTorch\n",
        "ov_fps_speedup = pt_metrics['time_sec'] / ov_metrics['time_sec']\n",
        "print(f\"\\nSpeedup with OpenVINO: {ov_fps_speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "System Info:\n",
            "Architecture:                    x86_64\n",
            "CPU op-mode(s):                  32-bit, 64-bit\n",
            "Byte Order:                      Little Endian\n",
            "Address sizes:                   46 bits physical, 48 bits virtual\n",
            "CPU(s):                          4\n",
            "On-line CPU(s) list:             0-3\n",
            "Thread(s) per core:              2\n",
            "Core(s) per socket:              2\n",
            "Socket(s):                       1\n",
            "NUMA node(s):                    1\n",
            "Vendor ID:                       GenuineIntel\n",
            "CPU family:                      6\n",
            "Model:                           85\n",
            "Model name:                      Intel(R) Xeon(R) Gold 6246R CPU @ 3.40GHz\n",
            "Stepping:                        7\n",
            "CPU MHz:                         3392.033\n",
            "BogoMIPS:                        6784.06\n",
            "Virtualization:                  VT-x\n",
            "Hypervisor vendor:               Microsoft\n",
            "Virtualization type:             full\n",
            "L1d cache:                       64 KiB\n",
            "L1i cache:                       64 KiB\n",
            "L2 cache:                        2 MiB\n",
            "L3 cache:                        35.8 MiB\n",
            "NUMA node0 CPU(s):               0-3\n",
            "Vulnerability Itlb multihit:     Not affected\n",
            "Vulnerability L1tf:              Not affected\n",
            "Vulnerability Mds:               Not affected\n",
            "Vulnerability Meltdown:          Not affected\n",
            "Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
            "Vulnerability Retbleed:          Vulnerable\n",
            "Vulnerability Spec store bypass: Vulnerable\n",
            "Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
            "Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\n",
            "Vulnerability Srbds:             Not affected\n",
            "Vulnerability Tsx async abort:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
            "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single tpr_shadow vnmi ept vpid ept_ad fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_vnni arch_capabilities\n",
            "\n",
            "\n",
            "System Memory Info (GB):\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:             82          22          46           0          13          59\n",
            "Swap:             0           0           0\n",
            "\n",
            "\n",
            "System OS:\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"20.04.6 LTS (Focal Fossa)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\n",
            "VERSION_ID=\"20.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=focal\n",
            "UBUNTU_CODENAME=focal\n",
            " \n",
            "Linux mir-user-pod-8bd02ecd3c9642a3ad90cefc96388ae6000000 5.15.0-1037-azure #44~20.04.1-Ubuntu SMP Mon Apr 24 21:52:51 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n",
            "\n",
            "/azureml-envs/azureml_6aa5ea01e3086a6204b0015abdeb760a/bin/python\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Print System info\n",
        "lscpu_out=output_dict['system_info']['lscpu_out'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Info:\\n{lscpu_out}\")\n",
        "\n",
        "mem_out_gb=output_dict['system_info']['mem_out_gb'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Memory Info (GB):\\n{mem_out_gb}\")\n",
        "\n",
        "os_out=output_dict['system_info']['os'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem OS:\\n{os_out}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Delete endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ml_client.online_endpoints.begin_delete(name=endpoint_name)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
